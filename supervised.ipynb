{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "Processing data and training both regular machine learning models as well as DL models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For local reproduction, please make sure to install pytorch with cuda compiled to enable GPU support.\n",
    "# https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install huggingface transformers for BERT\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install fastai which is used for LSTM\n",
    "# Jupyter has issues with installing fastai. We suggest to go outside of jupyter and \"pip install fastai\".\n",
    "# This will have a better chance of installing fastai v2 which we need.\n",
    "\n",
    "# Note: Fastai may require updating if used in colab.\n",
    "# Fastai doesn't work well unless using conda or linux. We suggest running the LSTM portion in colab.\n",
    "# Check the fastai documentation for installation instructions if any issue occurs: \n",
    "# You can comment out the below code to update it. Make sure the version is 2+.\n",
    "# ! [ -e /content ] && pip install -Uqq fastai\n",
    "import fastai\n",
    "fastai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycaret'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b8b9950740a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# For pycaret model tests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpycaret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# For DL models\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pycaret'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For supervised models\n",
    "from sklearn.model_selection import StratifiedKFold #stratified helps keep our dataset balanced\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer, KNNImputer\n",
    "\n",
    "# For pycaret model tests\n",
    "from pycaret.classification import *\n",
    "\n",
    "# For DL models\n",
    "# Based on data from pytorch and hugging face tutorials\n",
    "# https://www.thepythoncode.com/article/finetuning-bert-using-huggingface-transformers-python\n",
    "from fastai.text.all import *\n",
    "from fastai.tabular.all import *\n",
    "import torch\n",
    "from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-73586899b3e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m# ^^ safe to call this function even if cuda is not available\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mset_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-73586899b3e6>\u001b[0m in \u001b[0;36mset_seed\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mseed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \"\"\"\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "# Function copied from online tutorial to help reproducibility.\n",
    "# https://www.thepythoncode.com/article/finetuning-bert-using-huggingface-transformers-python\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch``\n",
    "\n",
    "    Args:\n",
    "        seed (int): The seed to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if is_torch_available():\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # ^^ safe to call this function even if cuda is not available\n",
    "\n",
    "set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running our code from Google Drive, you can connect to drive to load files quickly.\n",
    "# Uncomment out the below code to run. You may have to enter an authentication code.\n",
    "# Be aware that Google Colab often will fail due to memory requirements.\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imputed_array(df,from_save_path=None,save_to_path=None):\n",
    "    \"\"\"\n",
    "    Takes in a df of training data and returns an imputed numpy array.\n",
    "    Requires sklearn imputers.\n",
    "    \n",
    "    Args:\n",
    "    df -- The original X data dataframe\n",
    "    from_save_path -- Str of path where to load the saved file from if available\n",
    "    save_to_path -- Str of path where to save the imputed array to.\n",
    "    \n",
    "    Returns:\n",
    "    np.array -- Numpy array of imputed X data\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if from_save_path:\n",
    "        return np.load(from_save_path)\n",
    "    else:\n",
    "        arr = df.to_numpy()\n",
    "        arr_imputed = KNNImputer(missing_values=np.nan, add_indicator=False).fit_transform(arr)\n",
    "        if save_to_path:\n",
    "            with open(save_to_path, 'wb') as f:\n",
    "                np.save(f, arr_imputed)\n",
    "        return arr_imputed\n",
    "    \n",
    "def get_normalized_data(df, path,load=True):\n",
    "    \"\"\"\n",
    "    Load imputed numpy array and standardize it which is useful for traditional machine learning.\n",
    "    \n",
    "    Args:\n",
    "    df -- df to use in get imputed array function.\n",
    "    path -- path to load imputed array from\n",
    "    load -- boolean value. If set to true, function will load imputed array from path.\n",
    "    \n",
    "    Returns:\n",
    "    np.array -- Normalized & imputed numpy array of numerical features\n",
    "        \n",
    "    \"\"\"\n",
    "    if load:\n",
    "        X_imputed = get_imputed_array(df,from_save_path=path,save_to_path=None)\n",
    "        \n",
    "    # If we can't load it from storage, we will compute the array and then save it.\n",
    "    # Note: Resource intensive.\n",
    "    \n",
    "    else:\n",
    "        X_imputed = get_imputed_array(df,from_save_path=None,save_to_path=path)\n",
    "        \n",
    "    X_train_normalized = StandardScaler().fit_transform(X_imputed)\n",
    "    \n",
    "    return X_train_normalized\n",
    "\n",
    "def add_full_sentence_embedding(df,features,path=\"./pickles/X_train_s_imputed.npy\",load=False):\n",
    "    \"\"\"\n",
    "    Takes in a df, converts sentence embedding to individual columns, imputes the data and returnss the array.\n",
    "    \n",
    "    Args:\n",
    "    df -- main df to use. Must have sentence embedding column.\n",
    "    features -- list of relevant features\n",
    "    savepath -- path to save the numpy array to. Saves resources if running multiple times.\n",
    "    \n",
    "    Returns:\n",
    "    np.array -- Normalized and imputed numpy array including 50 columns related to the sentence embedding.\n",
    "    \"\"\"\n",
    "    # If load is set to true, load the imputed array from storage.\n",
    "    if load:\n",
    "        with open(path, 'rb') as f:\n",
    "            X_imputed_s_embed = np.load(f)\n",
    "    else:  \n",
    "        # Creating separate columns for our sentence embedding column\n",
    "        sentence_embed_df = pd.DataFrame(df[\"sentence_embed\"].to_list())\n",
    "\n",
    "        # Get the names of the columns. We have 50 because we used a 50 dimensional embedding.\n",
    "        embed_col_nums = [\"Embed-\"+str(i) for i in range(50)]\n",
    "        sentence_embed_df.columns = embed_col_nums\n",
    "\n",
    "        # Concat the sentence embed df with the original train df to add the columns back in\n",
    "        train_df_concat = pd.concat([df, sentence_embed_df], axis=1)\n",
    "\n",
    "        combined_cols = features + embed_col_nums\n",
    "\n",
    "        X_imputed_s_embed = get_imputed_array(train_df_concat[combined_cols],save_to_path=savepath)\n",
    "        \n",
    "    X_normalized_s_embed = StandardScaler().fit_transform(X_imputed_s_embed)\n",
    "    \n",
    "    return X_normalized_s_embed\n",
    "\n",
    "\n",
    "def add_kmeans_label(arr):\n",
    "    \"\"\"\n",
    "    Refit our KMeans label and add it to our numpy array as a feature.\n",
    "    \n",
    "    Args:\n",
    "    arr -- array to fit the kmeans data and produce the labels\n",
    "    \n",
    "    Returns:\n",
    "    np.array -- Numpy array with the kmeans predicted label column added\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters = 2,  random_state = 42)\n",
    "    kmeans.fit(arr)\n",
    "    X_norm_kmeans = np.append(arr,np.array(kmeans.labels_).reshape(-1,1),1)\n",
    "    return X_norm_kmeans\n",
    "\n",
    "def get_sample(X,y,num_samples=10000):\n",
    "    \"\"\"\n",
    "    Create samples which are useful for early tests and models that have resource limitations\n",
    "    \n",
    "    Args:\n",
    "    x -- training data features\n",
    "    y -- training data labels\n",
    "    num_samples -- number of samples to take from the full dataset\n",
    "    \n",
    "    Returns:\n",
    "    tuple -- tuple of X_sample array and y_sample array\n",
    "    \"\"\"\n",
    "    to_sample = np.append(X,y.reshape(-1,1),axis=1)\n",
    "    small_sample = resample(to_sample,n_samples=num_samples,stratify=y_train,random_state=42)\n",
    "    X_sample = small_sample[:,:-1]\n",
    "    y_sample = small_sample[:,-1]\n",
    "    return X_sample,y_sample\n",
    "\n",
    "def get_rf_feature_importances(X,y,features,n_estimators=100,max_depth=100,random_state=42,class_weight=\"balanced\"):\n",
    "    \"\"\"\n",
    "    Wrapper to train a random forest classifier and output the classifier, feature importances, and a graph of it.\n",
    "    \n",
    "    Args:\n",
    "    X -- Training data features\n",
    "    y -- Training labels to predict\n",
    "    features -- list of relevant features to use in training the model\n",
    "    n_estimators -- number of estimators for the random forest\n",
    "    max_depth -- mmax depth of the random forest\n",
    "    random_state -- int to set the random state value to help reduce randomization in results\n",
    "    class_weight -- str for setting the class weight of the random forest\n",
    "    \n",
    "    Returns:\n",
    "    clf_rf -- random forest classifier\n",
    "    features_df -- dataframe of feature importances\n",
    "    feature_chart -- feature importances chart\n",
    "    \n",
    "    \"\"\"\n",
    "    # Create classifier\n",
    "    clf_rf = RandomForestClassifier(n_estimators=n_estimators, \n",
    "                                    max_depth=max_depth,\n",
    "                                    random_state= random_state,\n",
    "                                    class_weight=class_weight)\n",
    "    # Fit it to data\n",
    "    clf_rf.fit(X,y)\n",
    "    \n",
    "    # Get the sorted index of the features based on their importance\n",
    "    index_sorted = clf_rf.feature_importances_.argsort()\n",
    "\n",
    "    # Get the sorted features using the index_sorted list\n",
    "    # We want the most important one first\n",
    "    features_sorted = [str(features[i]) for i in index_sorted]\n",
    "\n",
    "    \n",
    "    # Storing the features and their importance value in a dict to be returned\n",
    "    features_df = pd.DataFrame({\"feature\":features_sorted,\"importance\":clf_rf.feature_importances_[index_sorted]})\n",
    "    \n",
    "    # Plot the features\n",
    "    # Based on code from tutorials\n",
    "    \n",
    "    # Quick height adjustment to make the graph more visually appealing and easier to read\n",
    "    if (len(features)*20) < 600:\n",
    "        chart_height = 600\n",
    "    else:\n",
    "        chart_height = len(features)*20\n",
    "    \n",
    "    feature_chart = px.bar(features_df, \n",
    "                           x='importance', \n",
    "                           y='feature',\n",
    "                           title=\"Random Forest Feature Importance\",\n",
    "                           color=\"importance\",\n",
    "                           height=chart_height)\n",
    "    feature_chart.show()\n",
    "    \n",
    "    return clf_rf,features_df, feature_chart\n",
    "\n",
    "def plot_feature_accuracy(feature_importances,X_train,y_train,X_test,y_test,steps=1):\n",
    "    \"\"\"\n",
    "    Loops through df of top features and retrains a Random Forest model.\n",
    "    Each iteration adds the next most important feature.\n",
    "    Tracks the accuracy as more features are added and returns the accuracy df and chart.\n",
    "    \n",
    "    Args:\n",
    "    feature_importances -- dataframe of feature importances\n",
    "    X_train -- Feature training values. Used to train the Random Forest.\n",
    "    y_train -- Labels of training data. Used to train the Random Forest.\n",
    "    X_test -- Feature test values. Used to evaluate the Random Forest.\n",
    "    y_test -- Labels of test data. Used to evaluate the Random Forest.\n",
    "    \n",
    "    Returns:\n",
    "    acc_df -- dataframe of accuracy scores by the number of features used in training the Random Forest.\n",
    "    fig -- Plotly line chart of accuracy as number of features increases.\n",
    "    \"\"\"\n",
    "    accuracy_dict = {}\n",
    "    for i in tqdm(range(0,len(feature_importances),steps)):\n",
    "        feat_num = i+1\n",
    "#         print(features[-feat_num:])\n",
    "        X = X_train[feature_importances[-feat_num:][\"feature\"]]\n",
    "        X_t = X_test[feature_importances[-feat_num:][\"feature\"]]\n",
    "        \n",
    "        clf_rf = RandomForestClassifier(n_estimators=100,max_depth=100,random_state=42).fit(X,y_train)\n",
    "        \n",
    "        accuracy_dict[feat_num] = clf_rf.score(X_t,y_test)\n",
    "        print(f\"Accuracy for {feat_num} features: {accuracy_dict[feat_num]}\")\n",
    "        \n",
    "    acc_df = pd.DataFrame({\"Num Features\":accuracy_dict.keys(),\"Accuracy\":accuracy_dict.values()})\n",
    "    fig = px.line(acc_df,x=\"Num Features\",y=\"Accuracy\",title=\"Random Forest accuracy by number of features.<br>Top features first.\",width=600,height=500)\n",
    "    fig.show()\n",
    "    \n",
    "    return acc_df, fig\n",
    "\n",
    "def combine_text_features(df,features):\n",
    "    \"\"\"\n",
    "    Used for BERT models.\n",
    "    Concats our numerical features with the text data.\n",
    "    We separate them by a [SEP] token which is understood by BERT.\n",
    "    \n",
    "    Args:\n",
    "    df -- main dataframe\n",
    "    features -- list of relevant features to concatenate / combine\n",
    "    \n",
    "    Returns:\n",
    "    df -- dataframe with combined text column added\n",
    "    \"\"\"\n",
    "    \n",
    "    # Round our values to make them less unique. This will suit the token representation better.\n",
    "    df = df.round(2)\n",
    "    \n",
    "    # Get columns we need and convert to strings to concat with text.\n",
    "    df[features] = df[features].astype(str)\n",
    "    \n",
    "    # Add features to end of text.\n",
    "    # Adding [SEP] as a separator token for BERT models.\n",
    "    df[\"combined_text\"] = df[features].agg(\" [SEP] \".join,axis=1)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    \n",
    "class ComplexityDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Creates dataset based on complexity values.\n",
    "    From: https://www.thepythoncode.com/article/finetuning-bert-using-huggingface-transformers-python\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "def train_model_dl(df,training_args,model_path,model_name,label_names=[\"0\",\"1\"],test_size=0.05,max_length=64,bert=True):\n",
    "    \"\"\"\n",
    "    Wrapper function to process a df and train a deep learning model.\n",
    "    Will print out training metrics and save the model at the end.\n",
    "    \n",
    "    Based on HuggingFace tutorials as well as: \n",
    "    https://www.thepythoncode.com/article/finetuning-bert-using-huggingface-transformers-python\n",
    "    \n",
    "    Args:\n",
    "    df -- main dataframe of data. Should include combined_text and label columns.\n",
    "    training_args -- training argument object to pass to the trainer object in huggingface\n",
    "    model_path -- path to save the model after training\n",
    "    label_names -- our label names / values. In our case, 0 & 1.\n",
    "    test_size -- size of test data to evaluate on. Default is 5% but also used 10% in practice.\n",
    "    max_length -- maximum sequence length of the BERT model. Default is 64. Lower values produces less tokens and will truncate sentences.\n",
    "    bert -- boolean value. If set to true, will slightly optimize for standard BERT training.\n",
    "    \n",
    "    Returns:\n",
    "    model object -- HuggingFace pretrained BERT model that has been fine-tuned on our dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Train test split.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[\"combined_text\"],\n",
    "                                                        df[\"label\"],\n",
    "                                                        stratify=df[\"label\"], test_size=test_size, random_state=42)\n",
    "    \n",
    "    # The deep learning models need a list, not a series.\n",
    "    X_train = X_train.tolist()\n",
    "    X_test = X_test.tolist()\n",
    "    y_train = y_train.tolist()\n",
    "    y_test = y_test.tolist()\n",
    "\n",
    "    max_length = max_length\n",
    "    \n",
    "    # While the autotokenizer is fine, we can specify for BERT.\n",
    "    if bert:\n",
    "        tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "    try:\n",
    "        train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=max_length)\n",
    "        valid_encodings = tokenizer(X_test, truncation=True, padding=True, max_length=max_length)\n",
    "    except:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=max_length)\n",
    "        valid_encodings = tokenizer(X_test, truncation=True, padding=True, max_length=max_length)\n",
    "    \n",
    "    # convert our tokenized data into a torch Dataset\n",
    "    train_dataset = ComplexityDataset(train_encodings, y_train)\n",
    "    valid_dataset = ComplexityDataset(valid_encodings, y_test)\n",
    "    \n",
    "    # Init model and send to GPU\n",
    "    \n",
    "    if bert:\n",
    "        model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(label_names)).to(\"cuda\")\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name).to(\"cuda\")\n",
    "    \n",
    "    training_args = training_args\n",
    "    \n",
    "    trainer = Trainer(\n",
    "    model=model,                         # Transformers model\n",
    "    args=training_args,                  # training arguments\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=valid_dataset,          # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    trainer.evaluate()\n",
    "    \n",
    "    model_path = model_path\n",
    "    model.save_pretrained(model_path)\n",
    "    tokenizer.save_pretrained(model_path)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Computes metrics for use in our HuggingFace callbacks while training BERT models.\n",
    "    \n",
    "    Based on tutorial from: https://www.thepythoncode.com/article/finetuning-bert-using-huggingface-transformers-python\n",
    "    Customized to add F1 score\n",
    "    \n",
    "    Args:\n",
    "    pred -- predictions\n",
    "    \n",
    "    Returns:\n",
    "    dict -- dict of accuracy and f1 score metrics\n",
    "    \"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels,preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1':f1,\n",
    "    }\n",
    "\n",
    "def get_prediction(text,model):\n",
    "    \"\"\"\n",
    "    Function for computing predictions using our deep learning model\n",
    "    Based on tutorial: https://www.thepythoncode.com/article/finetuning-bert-using-huggingface-transformers-python\n",
    "    Customized to export prediction probability.\n",
    "    \n",
    "    Args:\n",
    "    text -- text to run through our model to predict a label\n",
    "    model -- model to use for prediction task\n",
    "    \n",
    "    Returns:\n",
    "    string -- name of predicted label\n",
    "    float -- probability score of label that was predicted\n",
    "    \"\"\"\n",
    "    \n",
    "    # prepare our text into tokenized sequence\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(\"cuda\")\n",
    "    # perform inference to our model\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # get output probabilities by doing softmax\n",
    "    probs = outputs[0].softmax(1)\n",
    "\n",
    "    # executing argmax function to get the candidate label\n",
    "    target_names = [0,1]\n",
    "    return target_names[probs.argmax()],float(probs.max())\n",
    "\n",
    "def dl_model_predict(model_path,model_name,bert=True):\n",
    "    \"\"\"\n",
    "    Wrapper function to process a df, load a DL model, and predict labels for it.\n",
    "    Based on Huggingface tutorials.\n",
    "    \n",
    "    Args:\n",
    "    model_path -- path to load the model from\n",
    "    model_name -- model to load. Used to initialize the tokenizer.\n",
    "    bert -- boolean to slightly optimize for BERT specific functions. Default is True.\n",
    "    \n",
    "    Returns:\n",
    "    huggingface pipeline object -- classifier built on huggingface pipeline object. Tokenizer + Model.\n",
    "    \"\"\"\n",
    "    \n",
    "    if bert:\n",
    "        tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Init model and send to GPU\n",
    "    \n",
    "    if bert:\n",
    "        model = BertForSequenceClassification.from_pretrained(model_path,local_files_only=True)\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path,local_files_only=True)\n",
    "        \n",
    "    classifier = pipeline(model=model, tokenizer=tokenizer,task=\"text-classification\")\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "def predict_batch(string_list,batch_size=64):\n",
    "    \"\"\"\n",
    "    Takes in a list of strings and predicts the labels using our model. Sometimes faster than pandas apply/\n",
    "    Batch helper code from stackoverflow\n",
    "    https://stackoverflow.com/questions/8290397/how-to-split-an-iterable-in-constant-size-chunks\n",
    "    \n",
    "    Args:\n",
    "    string_list -- list of strings to predict labels for.\n",
    "    batch_size -- size of batch. Higher number can be faster but more memory intensive. Default is 64.\n",
    "    \n",
    "    Returns:\n",
    "    list -- list of predictions dicts containing a predicted label and score for each string in the string list.\n",
    "    \"\"\"\n",
    "    def batch(iterable, n=1):\n",
    "        l = len(iterable)\n",
    "        for ndx in range(0, l, n):\n",
    "            yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "    pred_list = []\n",
    "\n",
    "    for x in tqdm(batch(test_strings, batch_size)):\n",
    "        pred_list.extend(bert_clas(x))\n",
    "        \n",
    "    return pred_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data & Process It\n",
    "\n",
    "We can load our imputed data or regular dataframes from our previous stage in our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features to be used in certain models\n",
    "features = [\"automated_readability_index\",\n",
    "            \"coleman_liau_index\",\n",
    "            \"flesch_kincaid_grade_level\",\n",
    "            \"flesch_reading_ease\",\n",
    "            \"gunning_fog_index\",\n",
    "            \"lix\",\n",
    "            \"perspicuity_index\",\n",
    "            \"smog_index\",\n",
    "            \"Passage Sum\",\n",
    "            \"Dale Chall Sum\",\n",
    "            \"Dale Chall Percent\",\n",
    "            \"SAT Sum\",\n",
    "            \"SAT Percent\",\n",
    "            \"AoA_Freq\",\n",
    "            \"AoA_Mean_Age\",\n",
    "            \"Conc.M\",\n",
    "            \"Percent_known\",\n",
    "            \"Average_Embed\",\n",
    "            \"TTR\",\n",
    "            \"Sqrd_AoA_Mean_Age\",\n",
    "            \"Max_AoA_Age\",\n",
    "            \"Min_AoA_Age\",\n",
    "            \"Pronoun Count\",\n",
    "            \"Pronoun Percent\",\n",
    "            \"commune_matched\",\n",
    "            \"football_matched\",\n",
    "            \"LRB_RRB_matched\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load main train df\n",
    "train_df = pickle.load(open('pickles/train_df.pkl','rb'))\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df[\"label\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_path = \"./pickles/X_imputed.npy\"\n",
    "X_norm_kmeans = add_kmeans_label(get_normalized_data(train_df[features],imputed_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample,y_sample = get_sample(X_norm_kmeans,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing the full 50 dimensional sentence embedding\n",
    "# Warning: Can be very resource intensive for only 1% improvement in accuracy.\n",
    "X_normalized_s_embed = add_full_sentence_embedding(train_df,features,path=\"./pickles/X_train_s_imputed.npy\",load=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Models - Traditional\n",
    "We trained multiple models to check how they performed. Most of the code here is based on scikit-learn tutorials or lecture material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting with a dummy classifier. Code is based on sklearn documentation.\n",
    "clf_dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "scores_dummy = cross_val_score(clf_dummy,X_norm_kmeans,y_train,cv=10)\n",
    "\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores_dummy.mean(), scores_dummy.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear kernel\n",
    "clf_svm = svm.SVC(kernel='linear', C=1, random_state=42)\n",
    "scores_svm = cross_val_score(clf_svm,X_sample,y_sample,cv=10)\n",
    "\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores_svm.mean(), scores_svm.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rbf kernel\n",
    "clf_svm = svm.SVC(kernel='rbf', C=1, random_state=42)\n",
    "scores_svm = cross_val_score(clf_svm,X_sample,y_sample,cv=10)\n",
    "\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores_svm.mean(), scores_svm.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poly kernel\n",
    "clf_svm = svm.SVC(kernel='poly', C=1, random_state=42)\n",
    "scores_svm = cross_val_score(clf_svm,X_sample,y_sample,cv=10)\n",
    "\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores_svm.mean(), scores_svm.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "clf_logreg = LogisticRegression(random_state=42)\n",
    "scores_logreg = cross_val_score(clf_logreg,X_norm_kmeans,y_train,cv=10)\n",
    "\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores_logreg.mean(), scores_logreg.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# RF cross val with normalized training data and kmeans label added\n",
    "clf_rf = RandomForestClassifier(n_estimators=100, max_depth=100, random_state=42, class_weight=\"balanced\")\n",
    "scores_rf_kmeans= cross_val_score(clf_rf,X_norm_kmeans,y_train,cv=10)\n",
    "\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores_rf_kmeans.mean(), scores_rf_kmeans.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This will run the Random Forest on the data with the 50 dimensional vectors. \n",
    "# Warning: It will take over an hour on most machines.\n",
    "\n",
    "# RF cross val with normalized training data and kmeans label added + 50 dimensional sentence embedding\n",
    "# clf_rf_50s = RandomForestClassifier(n_estimators=100, max_depth=100, random_state=42, class_weight=\"balanced\")\n",
    "# scores_rf_kmeans_50s= cross_val_score(clf_rf_50s,X_normalized_s_embed,y_train,cv=10)\n",
    "\n",
    "# print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores_rf_kmeans_50s.mean(), scores_rf_kmeans_50s.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Generates feature importance chart using the Random Forest\n",
    "features_kmeans = features + [\"kmeans_label\"]\n",
    "clf_rf_kmeans, clf_rf_kmeans_fidf, clf_rf_kmeans_chart = get_rf_feature_importances(X_norm_kmeans,\n",
    "                                                                                    y_train,\n",
    "                                                                                    features=features_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Plot the accuracy score as more top features are added.\n",
    "# Standard train test split used to generate data to help plot the accuracy over number of features.\n",
    "\n",
    "# Create kmeans df which will be useful for plotting.\n",
    "kmeans_df = pd.DataFrame(X_norm_kmeans,columns=features_kmeans)\n",
    "kmeans_df\n",
    "\n",
    "X_train, X_test, y_train_split, y_test_split = train_test_split(kmeans_df,\n",
    "                                                    y_train,\n",
    "                                                    stratify=y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "acc_df,acc_plot = plot_feature_accuracy(clf_rf_kmeans_fidf,X_train,y_train_split,X_test, y_test_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, our Random Forest was our best traditional machine learning model. We will see that it comes close to the deep learning models in terms of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyCaret Model Evaluations\n",
    "We used PyCaret as a secondary evaluation method. Code based on PyCaret tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for pycaret.\n",
    "pycaret_features = features + [\"label\"]\n",
    "pycaret_data = train_df[pycaret_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the classifier\n",
    "# Note: Sometimes categorizes numerical features as categorical. This can be easily solved by filtering those out.\n",
    "pycaret_classifier = setup(data = pycaret_data, target = 'label', session_id=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform model evaluation.\n",
    "best_model = compare_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyCaret also landed on Random Forest being the best overall model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Models - Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fastai code based on fastai tutorials. https://docs.fast.ai/tutorial.text.html\n",
    "# Load dataloaders\n",
    "dls = TextDataLoaders.from_df(train_df, text_col='original_text', label_col='label', valid_pct=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create learner object. This is a wrapper that handles training.\n",
    "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune LSTM.\n",
    "# We set the epochs to 4 and our learning rate to 1e-2.\n",
    "learn.fine_tune(4, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For exporting the model to save progress\n",
    "# learn.export(\"./pickles/fastai-lstm.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to rearrange our dataframes for getting data ready for the MLP\n",
    "\n",
    "features_tab = [\"automated_readability_index\",\n",
    "                \"coleman_liau_index\",\n",
    "                \"flesch_kincaid_grade_level\",\n",
    "                \"flesch_reading_ease\",\n",
    "                \"gunning_fog_index\",\n",
    "                \"lix\",\n",
    "                \"perspicuity_index\",\n",
    "                \"smog_index\",\n",
    "                \"Passage Sum\",\n",
    "                \"Dale Chall Sum\",\n",
    "                \"Dale Chall Percent\",\n",
    "                \"SAT Sum\",\n",
    "                \"SAT Percent\",\n",
    "                \"AoA_Freq\",\n",
    "                \"AoA_Mean_Age\",\n",
    "                \"Conc.M\",\n",
    "                \"Percent_known\",\n",
    "                \"Average_Embed\",\n",
    "                \"TTR\",\n",
    "                \"Sqrd_AoA_Mean_Age\",\n",
    "                \"Max_AoA_Age\",\n",
    "                \"Min_AoA_Age\",\n",
    "                \"Pronoun Count\",\n",
    "                \"Pronoun Percent\",\n",
    "                \"commune_matched\",\n",
    "                \"football_matched\",\n",
    "                \"LRB_RRB_matched\",\n",
    "                \"label\"\n",
    "]\n",
    "\n",
    "cat_names = [\"commune_matched\",\n",
    "             \"football_matched\",\n",
    "             \"LRB_RRB_matched\"\n",
    "]\n",
    "cont_names = [\"automated_readability_index\",\n",
    "              \"coleman_liau_index\",\n",
    "              \"flesch_kincaid_grade_level\",\n",
    "              \"flesch_reading_ease\",\n",
    "              \"gunning_fog_index\",\n",
    "              \"lix\",\n",
    "              \"perspicuity_index\",\n",
    "              \"smog_index\",\n",
    "              \"Passage Sum\",\n",
    "              \"Dale Chall Sum\",\n",
    "              \"Dale Chall Percent\",\n",
    "              \"SAT Sum\",\n",
    "              \"SAT Percent\",\n",
    "              \"AoA_Freq\",\n",
    "              \"AoA_Mean_Age\",\n",
    "              \"Conc.M\",\n",
    "              \"Percent_known\",\n",
    "              \"Average_Embed\",\n",
    "              \"TTR\",\n",
    "              \"Sqrd_AoA_Mean_Age\",\n",
    "              \"Max_AoA_Age\",\n",
    "              \"Min_AoA_Age\",\n",
    "              \"Pronoun Count\",\n",
    "              \"Pronoun Percent\",\n",
    "]\n",
    "procs = [Categorify, FillMissing, Normalize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MLP class had errors when the label was a number. \n",
    "# We replaced it with a boolean and went from 50% to 68% accuracy.\n",
    "mlp_df = train_df[features_tab].copy()\n",
    "mlp_df[\"label\"].replace({0: False, 1: True}, inplace=True)\n",
    "mlp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates splits\n",
    "splits = RandomSplitter(valid_pct=0.1)(range_of(mlp_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tabular object and dataloaders\n",
    "to = TabularPandas(mlp_df, procs=procs,\n",
    "                   cat_names = cat_names,\n",
    "                   cont_names = cont_names,\n",
    "                   y_names='label',\n",
    "                   splits=splits)\n",
    "\n",
    "dls = to.dataloaders(bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tabular learner\n",
    "learn_tab = tabular_learner(dls, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fastai has a useful function to estimate the best learning rate to use.\n",
    "learn_tab.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the data. We are not fine-tuning here. We are learning from our training data alone.\n",
    "learn_tab.fit_one_cycle(5,1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer (BERT)\n",
    "We fine-tuned a pre-trained BERT model from HuggingFace which gave us our best accuracy score.\n",
    "We attempted multiple different types of variants including regular BERT, distilbert, and RoBerta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_bert = [\"original_text\",\n",
    "                 \"automated_readability_index\",\n",
    "                 \"coleman_liau_index\",\n",
    "                 \"flesch_kincaid_grade_level\",\n",
    "                 \"flesch_reading_ease\",\n",
    "                 \"gunning_fog_index\",\n",
    "                 \"lix\",\n",
    "                 \"perspicuity_index\",\n",
    "                 \"smog_index\",\n",
    "                 \"Passage Sum\",\n",
    "                 \"Dale Chall Sum\",\n",
    "                 \"Dale Chall Percent\",\n",
    "                 \"SAT Sum\",\n",
    "                 \"SAT Percent\",\n",
    "                 \"AoA_Freq\",\n",
    "                 \"AoA_Mean_Age\",\n",
    "                 \"Conc.M\",\n",
    "                 \"Percent_known\",\n",
    "                 \"Average_Embed\",\n",
    "                 \"TTR\",\n",
    "                 \"Sqrd_AoA_Mean_Age\",\n",
    "                 \"Max_AoA_Age\",\n",
    "                 \"Min_AoA_Age\",\n",
    "                 \"Pronoun Count\",\n",
    "                 \"Pronoun Percent\",\n",
    "                 \"commune_matched\",\n",
    "                 \"football_matched\",\n",
    "                 \"LRB_RRB_matched\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat features together with original text to create a combined_text column\n",
    "combined_df = combine_text_features(train_df,features_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have written helpful wrapper functions in the above function section to make it easier to test and train models.\n",
    "# Most of the code is inspired from two sources:\n",
    "# HuggingFace Tutorials: https://huggingface.co/transformers/training.html\n",
    "# PythonCode Tutorial: https://www.thepythoncode.com/article/finetuning-bert-using-huggingface-transformers-python\n",
    "\n",
    "# First we set up our model by providing it training arguments.\n",
    "# This is where we can tune hyperparameters such as learning rate or weight decay.\n",
    "\n",
    "# We will also provide paths to save our model checkpoints.\n",
    "# Deep learning models take a long time to train, and the final result is often not the best.\n",
    "# It is useful to have checkpoints to load the most appropriate model for our task.\n",
    "# We are setting up the model to track accuracy and F1 score and load the best model at the end based\n",
    "# on accuracy score during the checkpoint evaluations\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results/bert-base-uncased-64_5-wd0.1_lr5e-5-r42',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=64,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=600,                # number of warmup steps for learning rate scheduler\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.1,               # strength of weight decay\n",
    "    logging_dir='./logs/bert-base-uncased-64_5-wd0.1_lr5e-5-r42',            # directory for storing logs\n",
    "    load_best_model_at_end=True,     # load the best model when finished training\n",
    "    metric_for_best_model=\"accuracy\", # load the best model based on accuracy due to match the Kaggle evaluation\n",
    "    greater_is_better=True,\n",
    "    eval_steps=2000,                 # when to evaluate the model\n",
    "    save_steps=2000,                 # when to save the model checkpoint\n",
    "    evaluation_strategy=\"steps\",     # evaluate each `logging_steps` (this can also be set to every epoch)\n",
    ")\n",
    "\n",
    "m_path = \"./models/bert-base-uncased-64_5-wd0.1_lr5e-5-r42\"\n",
    "m_name = \"bert-base-uncased\" # This is the model. More pretrained models can be found at https://huggingface.co/models\n",
    "bert64_1 = train_model_dl(combined_df, # The df to use as input\n",
    "                            training_args, # The arguments for the trainer\n",
    "                            m_path, # The path to the model for saving and loading if required\n",
    "                            model_name=m_name, # Name of model.\n",
    "                            label_names=[\"0\",\"1\"], # Label names or values. 0 and 1 for complexity labels.\n",
    "                            test_size=0.1, # Size of the test set to evaluate on based on the training data\n",
    "                            max_length=64, # Maximum sequence length. Lower number can help prevent overfitting.\n",
    "                            bert=True) # Tells our model that the model is using bert compared to others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model if required. This can load from a checkpoint too.\n",
    "model_path = \"./results/bert-base-uncased-64_10-wd0.1_3r1e-5/checkpoint-10000\"\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "# Get the classifier\n",
    "bert_clas = dl_model_predict(model_path=model_path,model_name=model_name,bert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test df (with engineered features)\n",
    "test_df = pickle.load(open('pickles/test_df_embed.pkl','rb'))\n",
    "test_df = combine_text_features(test_df,features_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tqdm.pandas() # Enables progress_apply which gives a loading bar for apply functions in pandas.\n",
    "test_df[\"preds\"] = test_df[\"original_text\"].progress_apply(lambda t: bert_clas(t)) # Creates column of predictions and scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate columns for predicted label and its probability score\n",
    "test_df[\"pred_label\"] = test_df[\"preds\"].apply(lambda x: x[\"label\"][-1])\n",
    "test_df[\"pred_score\"] = test_df[\"preds\"].apply(lambda x: x[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to pickle file\n",
    "test_df.to_pickle('./pickles/test_df_preds.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export predictions in CSV similar to the sample submission\n",
    "pred_path = \"\" # Put your file path here where you want to save the csv\n",
    "\n",
    "# Create ID columns\n",
    "test_df[\"ID\"] = test_df.index\n",
    "\n",
    "# Create df slice of just ID and predicted label for use in the sample submission\n",
    "preds = test_df[[\"ID\",\"pred_label\"]]\n",
    "\n",
    "preds.columns = [\"id\",\"label\"] # Make columns similar to the sample submission\n",
    "preds.to_csv(pred_path,index=False) # Don't export the index to make it match the sample submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data provided is from one of our BERT models.\n",
    "\n",
    "overfitting_df = {\n",
    "    \"Step\": [2000,4000,6000,8000,10000,12000,14000,16000,18000],\n",
    "    \"Training Loss\": [0.510100,0.486600,0.478400,0.415900,0.412200,0.407800,0.319000,0.314000,0.305900],\n",
    "    \"Validation Loss\": [0.505885,0.496645,0.472032,0.481597,0.473282,0.486976,0.531029,0.560407,0.570793],\n",
    "    \"Accuracy\": [0.740679,0.754307,0.763280,0.765872,0.769519,0.772206,0.767695,0.765248,0.767311],\n",
    "    \"F1\": [0.775972,0.778028,0.777562,0.773502,0.778367,0.787159,0.776490,0.771295,0.776286]\n",
    "}\n",
    "\n",
    "plt.plot(overfitting_df[\"Step\"], overfitting_df[\"Training Loss\"], label = \"Training Loss\")\n",
    "plt.plot(overfitting_df[\"Step\"], overfitting_df[\"Validation Loss\"], label = \"Validation Loss\")\n",
    "# plt.plot(overfitting_df[\"Step\"], overfitting_df[\"Accuracy\"], label = \"Accuracy\")\n",
    "# plt.plot(overfitting_df[\"Step\"], overfitting_df[\"Validation Loss\"], label = \"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training Loss vs. Validation Loss\")\n",
    "plt.suptitle(\"BERT Overfitting after 8K Steps\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have prepared a sample failure analysis dataframe from one of our earlier models.\n",
    "# This analysis inspired some features and ideas in our machine learning cycle.\n",
    "\n",
    "# Uncomment below to load it\n",
    "# compare_test_df = pickle.load(open('pickles/bert_compare_test_df.pkl','rb'))\n",
    "\n",
    "# If you don't wish to load this then please run the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split to mimic the same data that our BERT model was trained and evaluated on.\n",
    "# Make sure that the test_size is the same as the trainer size when training the model to avoid data leakage.\n",
    "X_train, X_test, y_train_fa, y_test_fa = train_test_split(combined_df[\"combined_text\"],\n",
    "                                                    combined_df[\"label\"],\n",
    "                                                    stratify=combined_df[\"label\"], test_size=0.1, random_state=42)\n",
    "\n",
    "# The deep learning models need a list, not a series.\n",
    "X_train = X_train.tolist()\n",
    "X_test = X_test.tolist()\n",
    "y_train_fa = y_train_fa.tolist()\n",
    "y_test_fa = y_test_fa.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data to check for predicted vs real labels. bert64_1 is the BERT model we previously trained above.\n",
    "# Note: The test data here is from the train test split, not the official Kaggle test set since that doesn't have labels.\n",
    "\n",
    "compare_test_df = pd.DataFrame({\"original_text\":X_test,\"real_label\":y_test_fa})\n",
    "compare_test_df[\"pred\"] = compare_test_df[\"original_text\"].apply(lambda x: get_prediction(x,bert64_1))\n",
    "compare_test_df[\"probability\"] = compare_test_df[\"pred\"].apply(lambda x: x[1])\n",
    "compare_test_df[\"correct\"] = compare_test_df[\"pred_label\"] == compare_test_df[\"real_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get incorrect and correct examples. Sort by highest probability first.\n",
    "# This shows us which predictions the model was most confident in.\n",
    "# High confidence wrong predictions are a key to analyzing what is going wrong or understanding complex situations.\n",
    "\n",
    "correct_test = compare_test_df[compare_test_df[\"correct\"] == True].sort_values(by=\"probability\",ascending=False)\n",
    "incorrect_test = compare_test_df[compare_test_df[\"correct\"] == False].sort_values(by=\"probability\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the full sentences in pandas. Check the top 10 high confidence correct predictions.\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(correct_test.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the full sentences in pandas. Check the top 10 high confidence incorrect predictions.\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(incorrect_test.head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During our analysis, we noticed certain trends. A lot of the incorrect but high confidence predictions involved words like football, commune, or LRB / RRB tags. This inspired some of our features, as well as gave us insight into our topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
